# Visual Transformers Captioning, improving captioning for Blind & Visually Impaired People

This is an independent research project to test the efficacy of Visual Transformers (Wu et al), in combination with a normal transformer, for captioning. The Visual Transformer performs feature extraction. The transformer then uses the extracted features to produce a caption. This research was mainly aimed at assisting people with vision disabilities, which is why I used the VizWiz Image Captioning dataset (https://vizwiz.org/tasks-and-datasets/image-captioning/) to train the language model.

Visual Transformer paper: https://arxiv.org/abs/2006.03677

Visual Transformer code from: (https://github.com/NazirNayal8/visual-transformer)

## Visual Diagrams of Model
### Model Overview
![image_of_model](./model_diagram.png)
